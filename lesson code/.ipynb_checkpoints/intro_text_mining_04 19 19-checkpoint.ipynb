{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caesar Ciper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://www.maths-resources.net/enrich/codes/caesar/images/caesarwheel3.gif)\n",
    "\n",
    "The letters on the outer circle represent letters in the original text (message). The letters on the inner circle represent the (encoded) cipher text.\n",
    "\n",
    "Here, the inner circle is rotated to the left by 3 (`k`=3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `chr()` and `ord()` Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ord(c)`: Returns an integer representing the Unicode code point of the character `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chr(i)`: Returns a string of one character whose ASCII code is the integer `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "chr(65+k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(chr(65+k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ord()` and `chr()` functions are the opposite of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caesar cipher in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the message text, and key\n",
    "\n",
    "message = 'Et tu, Brute?'\n",
    "\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The message is: Et tu, Brute?\n",
      "The key is: 3\n"
     ]
    }
   ],
   "source": [
    "print ('The message is:', message)\n",
    "\n",
    "print('The key is:', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encrypt the message, one letter at a time\n",
    "\n",
    "for letter in message:\n",
    "    \n",
    "    print ('Input letter:', letter)\n",
    "    \n",
    "    # Retrieve the ASCII code for the letter\n",
    "    num = ord(letter)\n",
    "    \n",
    "    # Add 'k' to that code\n",
    "    num = num + k\n",
    "    \n",
    "    # Retrieve the letter for that ASCII code\n",
    "    encrypted_letter = chr(num)\n",
    "    \n",
    "    print ('Encrypted letter:', encrypted_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encrypt only letters in the alphabet, and ignore special characters, e.g., space, exclamation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore special characters\n",
    "\n",
    "for letter in message:\n",
    "    \n",
    "    print ('Input letter:', letter)\n",
    "    \n",
    "    if letter.isalpha():\n",
    "        num = ord(letter)\n",
    "        num += k\n",
    "        encrypted_letter = chr(num)\n",
    "    else:\n",
    "        encrypted_letter = letter\n",
    "    \n",
    "    print ('Encrypted letter:', encrypted_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `n = n + k` can also be written as `n += k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encypted message in a string\n",
    "\n",
    "# Initialize the output (encrypted) message\n",
    "encrypted_message = ''\n",
    "\n",
    "for letter in message:\n",
    "    \n",
    "    if letter.isalpha():\n",
    "        num = ord(letter)\n",
    "        num += k\n",
    "        encrypted_message += chr(num)\n",
    "    else:\n",
    "        encrypted_message += letter\n",
    "\n",
    "print ('Input message:', message)\n",
    "print ('Encrypted message:', encrypted_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_message(in_message):\n",
    "\n",
    "    ## -- INSERT CODE HERE -- ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to encrypt the message\n",
    "encrypted_message = encrypt_message(message)\n",
    "\n",
    "# Print the encrypted message\n",
    "print ('Encrypted message:', encrypted_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the function to include</i> `k` <i>as one of its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_message(in_message, key):\n",
    "\n",
    "    # initialize the output (encrypted) message\n",
    "    out_message = ''\n",
    "\n",
    "    for letter in in_message:\n",
    "        if letter.isalpha():\n",
    "            num = ord(letter)\n",
    "            num += key\n",
    "            out_message += chr(num)\n",
    "        else:\n",
    "            out_message += letter\n",
    "\n",
    "    return out_message\n",
    "\n",
    "print ('Encrypted message (k=3):', encrypt_message(message, 3))\n",
    "print ('Encrypted message (k=7):', encrypt_message(message, 7))\n",
    "print ('Encrypted message (k=0):', encrypt_message(message, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `k=7`, letter w would get encrypted into the curly bracket symbol. See the ASCII table below for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](http://www.asciitable.com/index/asciifull.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the function to avoid situations where the encrypted message contains non-alphabetic character(s).\n",
    "In other words, force the encryption to \"wrap around\" to the beginning of the alphabet if it encounters non-alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_message(in_message, key):\n",
    "\n",
    "    # Initialize the output (encrypted) message\n",
    "    out_message = ''\n",
    "\n",
    "    for letter in in_message:\n",
    "        \n",
    "        if letter.isalpha():\n",
    "            num = ord(letter)\n",
    "            num += key\n",
    "            \n",
    "            if letter.isupper() and num > ord('Z'):\n",
    "                num -= 26\n",
    "            elif letter.islower() and num > ord('z'):\n",
    "                num -= 26\n",
    "            \n",
    "            out_message += chr(num)\n",
    "        else:\n",
    "            out_message += letter\n",
    "\n",
    "    return out_message\n",
    "\n",
    "print ('Encrypted message (k=3):', encrypt_message(message, 3))\n",
    "print ('Encrypted message (k=3):', encrypt_message(message, 7))\n",
    "print ('Encrypted message (k=3):', encrypt_message(message, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to decode an encrypted message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_message(in_message, key):\n",
    "\n",
    "    # Initialize the output (encrypted) message\n",
    "    out_message = ''\n",
    "\n",
    "    for letter in in_message:\n",
    "        \n",
    "        if letter.isalpha():\n",
    "            num = ord(letter)\n",
    "            \n",
    "            # For decrypting the message, we need to substract the key\n",
    "            num -= key\n",
    "                \n",
    "            out_message += chr(num)\n",
    "        else:\n",
    "            out_message += letter\n",
    "\n",
    "    return out_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_message = 'Hw wx, Euxwh?'\n",
    "\n",
    "print ('Decrypted message:', decrypt_message(encrypted_message, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we don't know the key that was used to encrypt the message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use brute force to decode the message\n",
    "# Try all possible values of k (1 to 26)\n",
    "\n",
    "for k in range(1, 26):\n",
    "    print (decrypt_message(encrypted_message, k), f'(key={k})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the `decrypt_message()` function to avoid situations where the decrypted messages contain non-alphabetic character(s).\n",
    "In other words, force the decryption to \"wrap around\" to the beginning of the alphabet if it encounters non-alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_message(in_message, key):\n",
    "\n",
    "    # Initialize the output (encrypted) message\n",
    "    out_message = ''\n",
    "\n",
    "    for letter in in_message:\n",
    "        if letter.isalpha():\n",
    "            num = ord(letter)\n",
    "            num -= key\n",
    "            \n",
    "            if letter.isupper():\n",
    "                if num > ord('Z'):\n",
    "                    num -= 26\n",
    "                elif num < ord('A'):\n",
    "                    num += 26\n",
    "            elif letter.islower():\n",
    "                if num > ord('z'):\n",
    "                    num -= 26\n",
    "                elif num < ord('a'):\n",
    "                    num += 26\n",
    "            \n",
    "            out_message += chr(num)\n",
    "        else:\n",
    "            out_message += letter\n",
    "\n",
    "    return out_message\n",
    "\n",
    "for k in range(1, 26):\n",
    "    print (decrypt_message(encrypted_message, k), f'(key={k})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code in this exercise is adopted from [*Invent Your Own Computer Games with Python* by Al Sweigart](http://a.co/d/4LAcqtI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Speech: \"Friends, Romans, countrymen, lend me your ears\" BY WILLIAM SHAKESPEARE](https://www.poetryfoundation.org/poems/56968/speech-friends-romans-countrymen-lend-me-your-ears)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = 'Friends, Romans, countrymen, lend me your ears; \\\n",
    "I come to bury Caesar, not to praise him. \\\n",
    "The evil that men do lives after them; \\\n",
    "The good is oft interred with their bones; \\\n",
    "So let it be with Caesar. The noble Brutus \\\n",
    "Hath told you Caesar was ambitious: \\\n",
    "If it were so, it was a grievous fault, \\\n",
    "And grievously hath Caesar answer’d it. \\\n",
    "Here, under leave of Brutus and the rest– \\\n",
    "For Brutus is an honourable man; \\\n",
    "So are they all, all honourable men– \\\n",
    "Come I to speak in Caesar’s funeral. \\\n",
    "He was my friend, faithful and just to me: \\\n",
    "But Brutus says he was ambitious; \\\n",
    "And Brutus is an honourable man. \\\n",
    "He hath brought many captives home to Rome \\\n",
    "Whose ransoms did the general coffers fill: \\\n",
    "Did this in Caesar seem ambitious? \\\n",
    "When that the poor have cried, Caesar hath wept: \\\n",
    "Ambition should be made of sterner stuff: \\\n",
    "Yet Brutus says he was ambitious; \\\n",
    "And Brutus is an honourable man. \\\n",
    "You all did see that on the Lupercal \\\n",
    "I thrice presented him a kingly crown, \\\n",
    "Which he did thrice refuse: was this ambition? \\\n",
    "Yet Brutus says he was ambitious; \\\n",
    "And, sure, he is an honourable man. \\\n",
    "I speak not to disprove what Brutus spoke, \\\n",
    "But here I am to speak what I do know. \\\n",
    "You all did love him once, not without cause: \\\n",
    "What cause withholds you then, to mourn for him? \\\n",
    "O judgment! thou art fled to brutish beasts, \\\n",
    "And men have lost their reason. Bear with me; \\\n",
    "My heart is in the coffin there with Caesar, \\\n",
    "And I must pause till it come back to me.'\n",
    "\n",
    "print (encrypt_message(speech, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count letter frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Covert all letters to uppercase and then count the frequency\n",
    "letters = collections.Counter(speech.upper())\n",
    "\n",
    "letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the results in a descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_letters = []\n",
    "sorted_freq = []\n",
    "\n",
    "sorted_dict = sorted(letters.items(), key=lambda val: val[1], reverse=True)\n",
    "\n",
    "for key, val in sorted_dict:\n",
    "    if key.isalpha():\n",
    "        sorted_letters.append(key)\n",
    "        sorted_freq.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sorted_letters, sorted_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure().set_size_inches(12, 9)\n",
    "\n",
    "plt.bar(sorted_letters, sorted_freq)\n",
    "plt.xlabel('Letter', size=14)\n",
    "plt.ylabel('Frequency', size=14)\n",
    "plt.xticks(sorted_letters)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(speech.split())\n",
    "\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Also see:_ ####\n",
    "    \n",
    "[Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law): Given a large sample of words used, the frequency of any word is inversely proportional to its rank in the frequency table. So word number n has a frequency proportional to 1/n. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to deal with text documents is to first convert them into a numeric vector form (sparse matrix), and then perform additional analysis -- like clsutering, classification, and visualization -- using those vectors. This is usually referred to as 'Bag-of-Words' or 'Vector Space Model'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_punctuations = set(string.punctuation)\n",
    "\n",
    "all_punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_clean = ''.join(l for l in speech if l not in all_punctuations)\n",
    "\n",
    "speech_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cover to Upper/Lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_clean = speech_clean.lower()\n",
    "\n",
    "speech_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` package provides a list of stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discard all stop words from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_words = [word for word in speech_clean.split() if word not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "set(speech_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflected or derived words to their word stem, base or root form. There are several stemming algorithms available; we will use *Porter* and *Lancaster* stemmers in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a stemmer\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "# Example 1\n",
    "for word in ['Play', 'Playing', 'Played']:\n",
    "    \n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    print ('Word:', word, '\\t --> Stem:', stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "\n",
    "for word in ['grievous', 'grievously']:\n",
    "    \n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    print ('Word:', word, ' \\t --> Stem:', stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Define\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "for word in ['grievous', 'grievously']:\n",
    "    \n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    print ('Word:', word, ' \\t --> Stem:', stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply stemmer on the speech text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array to store the results (i.e., stems)\n",
    "stems = []\n",
    "\n",
    "for word in speech_words:\n",
    "    \n",
    "    # Check if it's a stop word\n",
    "    if word not in ENGLISH_STOP_WORDS:\n",
    "        \n",
    "        # Append the stem for each word to the output array\n",
    "        stems.append(stemmer.stem(word))\n",
    "   \n",
    "set(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: [Julie Beth Lovins](https://en.wikipedia.org/wiki/Julie_Beth_Lovins), a computational linguist, published the first-ever stemming algorithm in 1968."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Lemmatization, we need a dictionary, or a corpus reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in ['know', 'knowing', 'knew', 'knowledge']:\n",
    "    \n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    print ('Word:', word, '--> Stem:', stem, '--> Lemma:', lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must provide the context in which we're trying to lemmatize the words. This is refered to as the Parts-Of-Speech (POS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in ['know', 'knowing', 'knew', 'knowledge']:\n",
    "    \n",
    "    # Adding pos argument to lemmatize()\n",
    "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    print ('Word:', word, '--> Stem:', stem, '--> Lemma:', lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array to store the results (i.e., lemmas)\n",
    "lemmas = []\n",
    "\n",
    "for word in speech_words:\n",
    "    \n",
    "    # Check if it's a stop word\n",
    "    if word not in ENGLISH_STOP_WORDS:\n",
    "        \n",
    "        # Append the stem for each word to the output array\n",
    "        lemmas.append(lemmatizer.lemmatize(word, 'v'))\n",
    "    \n",
    "set(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speech_clean.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are closely related. Unlike Lemmatization, Stemming doesn't incorporate the conext (part of speech) but they typically run faster. In Information Retrieval applications, Stemming improves the True Positive Rate (recall), but reduces the True Negative Rate (specificity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next part of this exercise, let's analyze transcripts from a couple of US presidential inagural addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# NLTK tokenizer (to split a sentence into words)\n",
    "nltk.download('punkt')\n",
    "\n",
    "trump_speech_transcript = r\"C:\\Users\\visha\\derive Dropbox\\Projects\\vcu\\python\\misc\\inaugural_speech_trump.txt\"\n",
    "obama_speech_transcript = r\"C:\\Users\\visha\\derive Dropbox\\Projects\\vcu\\python\\misc\\inaugural_speech_obama.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokens(infile):\n",
    "    \n",
    "    with open(infile) as f:\n",
    "        \n",
    "        # Read each line from the file and convert it into lowercase\n",
    "        line = f.read().lower()\n",
    "\n",
    "        # Remove all punctuations\n",
    "        line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "        \n",
    "        # Remove all stop words (this will create a list of words)\n",
    "        line_words = [word for word in line_clean.split() if word not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "        # Join all those words to create a line (of text) again\n",
    "        line_clean = ' '.join(line_words)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(line_clean)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "tokens = create_tokens(trump_speech_transcript)\n",
    "\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(tokens)\n",
    "\n",
    "print (count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_tokens(infile):\n",
    "    \n",
    "    with open(infile) as f:\n",
    "        \n",
    "        # Read each line from the file and convert it into lowercase\n",
    "        line = f.read().lower()\n",
    "\n",
    "        # Remove all punctuations\n",
    "        line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "        \n",
    "        # Remove all stop words (this will create a list of words)\n",
    "        # In addition, use regex to replace \n",
    "        line_words = [re.sub(\"[^a-zA-Z' ]+\", '', word) for word in line_clean.split() if word not in ENGLISH_STOP_WORDS]\n",
    "        \n",
    "        # Join all those words to create a line (of text) again\n",
    "        line_clean = ' '.join(line_words)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(line_clean)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "tokens_trump = create_tokens(trump_speech_transcript)\n",
    "\n",
    "token_count_trump = Counter(tokens_trump)\n",
    "\n",
    "print (token_count_trump.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For an explanation of how that `regex` query replaces all non-letter chatacters with '' (nothing), please follow this [link](https://stackoverflow.com/questions/47561298/python-regex-remove-escape-characters-and-punctuation-except-for-apostrophe?rq=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_obama = create_tokens(obama_speech_transcript)\n",
    "\n",
    "token_count_obama = Counter(tokens_obama)\n",
    "\n",
    "print (token_count_obama.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokens(infile):\n",
    "    \n",
    "    with open(infile) as f:\n",
    "        \n",
    "        # Read each line from the file and convert it into lowercase\n",
    "        line = f.read().lower()\n",
    "\n",
    "        # Remove all punctuations\n",
    "        line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "        \n",
    "        # Remove all stop words (this will create a list of words)\n",
    "        # In addition, use regex to replace \n",
    "        line_words = [re.sub(\"[^a-zA-Z' ]+\", '', word) for word in line_clean.split() if word not in ENGLISH_STOP_WORDS\n",
    "                     and word != 'applause']\n",
    "\n",
    "        # Join all those words to create a line (of text) again\n",
    "        line_clean = ' '.join(line_words)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(line_clean)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "tokens_obama = create_tokens(obama_speech_transcript)\n",
    "\n",
    "token_count_obama = Counter(tokens_obama)\n",
    "\n",
    "print (token_count_obama.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency – Inverse Document Frequency. The idea behind this metric is to rescale the frequency of each word by how often they appear across all documents. Words that are common across all documents are penalized, and as a result, the words that are most distinct (and ferquent) within a document are emphasized more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokens_all = {}\n",
    "\n",
    "with open(trump_speech_transcript) as f:\n",
    "\n",
    "    # Read each line from the file and convert it into lowercase\n",
    "    line = f.read().lower()\n",
    "\n",
    "    # Remove all punctuations\n",
    "    line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "\n",
    "    # Remove all stop words (this will create a list of words)\n",
    "    # In addition, use regex to remove non-alphabetic characters\n",
    "    line_words = [re.sub(\"[^a-zA-Z' ]+\", '', word) for word in line_clean.split() if word not in ENGLISH_STOP_WORDS\n",
    "                 and word != 'applause']\n",
    "\n",
    "    # Join all those words to create a line (of text) again\n",
    "    line_clean = ' '.join(line_words)\n",
    "\n",
    "    tokens_all['trump'] = line_clean\n",
    "    \n",
    "with open(obama_speech_transcript) as f:\n",
    "\n",
    "    # Read each line from the file and convert it into lowercase\n",
    "    line = f.read().lower()\n",
    "\n",
    "    # Remove all punctuations\n",
    "    line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "\n",
    "    # Remove all stop words (this will create a list of words)\n",
    "    # In addition, use regex to replace \n",
    "    line_words = [re.sub(\"[^a-zA-Z' ]+\", '', word) for word in line_clean.split() if word not in ENGLISH_STOP_WORDS\n",
    "                 and word != 'applause']\n",
    "\n",
    "    # Join all those words to create a line (of text) again\n",
    "    line_clean = ' '.join(line_words)\n",
    "\n",
    "    tokens_all['obama'] = line_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfs_matrix = tfidf.fit_transform(tokens_all.values())\n",
    "\n",
    "print(tfs_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is how a sparse matrix is represented in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature (\"column\") names\n",
    "\n",
    "print(tfidf.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert from sparse matrix to dense matrix\n",
    "\n",
    "tfs_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "scores = tfs_matrix.todense().tolist()\n",
    "\n",
    "df = pd.DataFrame(scores, columns=feature_names, index=['trump', 'obama'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count_trump.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, c in token_count_trump.most_common(10):\n",
    "    print (w, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_trump = []\n",
    "\n",
    "for w, c in token_count_trump.most_common(10):\n",
    "    words_trump.append(w)\n",
    "    \n",
    "words_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[words_trump].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_obama = []\n",
    "\n",
    "for w, c in token_count_obama.most_common(10):\n",
    "    words_obama.append(w)\n",
    "    \n",
    "words_obama\n",
    "\n",
    "df[words_obama].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text from Web-pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following web-site contains US Presidential inauguration speeches: http://avalon.law.yale.edu/subject_menus/inaug.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `requests` and `BeautifulSoup` packages to read data directly from this web-site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"http://avalon.law.yale.edu/21st_century/obama.asp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Ping the web-page for information. This called making a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Use Beautiful Soup to parse the document using the best available parser. It will use an HTML parser unless you specifically tell it to use an XML parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source_code.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the content\n",
    "\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Extract the text part of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_obama = soup.get_text()\n",
    "\n",
    "speech_obama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Extract a specific portion of the text chunk which contains the actual speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the speech starts with 'My fellow citizens', which is immediately preceeded by the following: `\\r\\n\\n\\n\\n`. Let's split the text chunk into two parts using `\\r\\n\\n\\n\\n` as the separator, and then take the second half of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_obama = speech_obama.split('\\r\\n\\n\\n\\n')[1]\n",
    "\n",
    "speech_obama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the speech actually ends with 'And God bless the United States of America.', which is immediately followed by `\\n\\n\\n\\n\\n`. Let's split the text chunk into two parts using `\\n\\n\\n\\n\\n` as the separator, and then take the *first* half of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_obama = speech_obama.split('\\n\\n\\n\\n')[0]\n",
    "\n",
    "speech_obama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Clean and Tokenize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each line from the file and convert it into lowercase\n",
    "line = speech_obama.lower()\n",
    "\n",
    "# Remove all punctuations\n",
    "line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "\n",
    "# Remove all stop words (this will create a list of words)\n",
    "line_words = [word for word in line_clean.split() if word not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "# Join all those words to create a line (of text) again\n",
    "line_clean = ' '.join(line_words)\n",
    "\n",
    "# Tokenize\n",
    "tokens = nltk.word_tokenize(line_clean)\n",
    "\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applications of Text Mining:**\n",
    "    \n",
    "    1. Text (or Document) Categorization\n",
    "    2. Text Clustering\n",
    "    3. Sentiment Analysis\n",
    "    4. Document Summarization\n",
    "    5. Topic Extraction\n",
    "    6. Document Associations \n",
    "    7. Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources:**\n",
    "    \n",
    "1. [NLTK 3.4 documentation](http://www.nltk.org/index.html)\n",
    "2. [spaCy API](https://spacy.io/api)\n",
    "3. [scikit-learn TF-IDF Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "4. [NLTK's WordNet Interface](http://www.nltk.org/howto/wordnet.html)\n",
    "5. [Modern NLP in Python by Patrick Harrison | PyData DC 2016](https://www.youtube.com/watch?v=6zm9NC9uRkk) (YouTube)\n",
    "6. [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/) (Book)\n",
    "7. [Text Feature Extraction using scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
