{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use the \"Internet Advertisements\" data set from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Internet+Advertisements). \n",
    "\n",
    "This dataset represents a set of possible advertisements on Internet pages. The features encode the geometry of the image (if available) as well as phrases occuring in the URL, the image's URL and alt text, the anchor text, and words occuring near the anchor text. The task is to predict whether an image is an advertisement (\"ad\") or not (\"nonad\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the output files\n",
    "path = r'C:\\Users\\jonathan\\Desktop'\n",
    "\n",
    "data_file = path + r'\\ad.data'\n",
    "header_file = path + r'\\ad.names'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, header=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_text = open(header_file, \"r\")\n",
    "\n",
    "lines = header_text.read().split('\\n')\n",
    "\n",
    "lines[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = []\n",
    "\n",
    "for line in lines[4:-1]:\n",
    "    \n",
    "    if line[0] not in ('|', ''):\n",
    "        header.append(line.split(':')[0])\n",
    "        \n",
    "header[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header.append('ad')\n",
    "\n",
    "target = 'ad'\n",
    "\n",
    "df.columns = header\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the target variables from strict to numeric (binary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target] = [1 if x == 'ad.' else 0 for x in df[target]]\n",
    "\n",
    "df[target].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.height.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the four continuous variables into numeric (float). The code below will replace all '?' with nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(target, axis=1)\n",
    "\n",
    "all_vars = X.columns \n",
    "\n",
    "y = df[target]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = X.describe(percentiles=[.98, .99])\n",
    "\n",
    "desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = desc.T\n",
    "\n",
    "desc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc['pct_missing'] = 1 - desc['count'] / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc['kurtosis'] = X.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corr, all_corr_abs = [], []\n",
    "\n",
    "for var in all_vars:\n",
    "    \n",
    "    all_corr.append(X[var].corr(y))\n",
    "    all_corr_abs.append(abs(X[var].corr(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc['corr'] = all_corr\n",
    "\n",
    "desc['abs_corr'] = all_corr_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc.to_csv(path + r'\\desc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two (non-binary) variables with kurtosis 10 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_vars = ['height', 'aratio']\n",
    "\n",
    "for trim_var in trim_vars:\n",
    "    \n",
    "    print (trim_var)    \n",
    "    p99 = desc.loc[trim_var, '99%']\n",
    "    \n",
    "    X[trim_var] = [min(x, p99) for x in X[trim_var]]\n",
    "    \n",
    "    print ('Kurtosis --> before', desc.loc[trim_var, 'kurtosis'], 'after:', X[trim_var].kurtosis()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_vars = ['aratio']\n",
    "\n",
    "for trim_var in trim_vars:\n",
    "    \n",
    "    p98 = desc.loc[trim_var, '98%']\n",
    "    \n",
    "    X[trim_var] = [min(x, p98) for x in X[trim_var]]\n",
    "    \n",
    "    print ('Kurtosis --> before', desc.loc[trim_var, 'kurtosis'], 'after:', X[trim_var].kurtosis()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_vars = ['height', 'width', 'aratio', 'local']\n",
    "\n",
    "for impute_var in impute_vars:\n",
    "    \n",
    "    # Create missing value indicator\n",
    "    X[impute_var + '_M'] = [1 if pd.isnull(x) else 0 for x in X[impute_var]]\n",
    "    \n",
    "    median = desc.loc[impute_var, '50%']\n",
    "    \n",
    "    X[impute_var] = X[impute_var].fillna(median)\n",
    "    \n",
    "X[impute_vars].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for impute_var in impute_vars:\n",
    "    print (impute_var, X[impute_var].corr(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xform_vars = ['height', 'width', 'aratio']\n",
    "\n",
    "# For this exercise, we will try log transformation\n",
    "\n",
    "for xform_var in xform_vars:\n",
    "    \n",
    "    print (xform_var)\n",
    "    \n",
    "    log_x = np.log(X[xform_var])\n",
    "    \n",
    "    orig_abs_corr = desc.loc[impute_var, 'abs_corr']\n",
    "    \n",
    "    new_abs_corr = abs(y.corr(log_x))\n",
    "    \n",
    "    corr_pct_improvement = (new_abs_corr - orig_abs_corr) / orig_abs_corr\n",
    "    \n",
    "    print ('Original: ', orig_abs_corr, 'New: ', new_abs_corr, 'Improvement: ', corr_pct_improvement)\n",
    "    \n",
    "    if corr_pct_improvement >= 0.05:\n",
    "        \n",
    "        X[xform_var + '_log'] = [np.log(x) for x in X[xform_var]]\n",
    "        \n",
    "        X.drop(xform_var, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "\n",
    "plt.subplots(figsize = (12, 9))\n",
    "\n",
    "plt.plot(range(len(desc)), desc['std'])\n",
    "\n",
    "plt.xlabel('Attributes', fontsize=14)\n",
    "plt.ylabel('Standard Deviation', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (12, 9))\n",
    "\n",
    "plt.plot(range(len(desc[desc['max']==1])), sorted(desc[desc['max']==1]['std'], reverse=True))\n",
    "\n",
    "plt.xlabel('Binary Attributes', fontsize=14)\n",
    "plt.ylabel('Standard Deviation', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (12, 9))\n",
    "\n",
    "plt.plot(range(len(desc[desc['std']<.1])), sorted(desc[desc['std']<.1]['std'], reverse=True))\n",
    "\n",
    "plt.xlabel('Binary Attributes', fontsize=14)\n",
    "plt.ylabel('Standard Deviation', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = X.describe().T\n",
    "\n",
    "core_vars = desc.index[desc['std'] > 0.055]\n",
    "\n",
    "len(core_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corr_abs = {}\n",
    "\n",
    "for var in core_vars:\n",
    "\n",
    "    all_corr_abs[var] = abs(X[var].corr(y))\n",
    "    \n",
    "all_corr_abs_df = pd.DataFrame.from_dict([all_corr_abs]).T\n",
    "\n",
    "all_corr_abs_df.columns=['abs_corr']\n",
    "\n",
    "all_corr_abs_df['rank'] = all_corr_abs_df.rank(ascending=False) \n",
    "\n",
    "all_corr_abs_df = all_corr_abs_df.sort_values(by='rank')\n",
    "\n",
    "all_corr_abs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (12, 9))\n",
    "\n",
    "plt.plot(all_corr_abs_df['rank'], all_corr_abs_df['abs_corr'])\n",
    "\n",
    "plt.xlabel('Attribute Rank', fontsize=14)\n",
    "plt.ylabel('Correlation with Target', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will not be performing multi-collinearity analysis in this exercise to further reduce the number of features, we will use a more aggressive threshold to drop variables that have a low correlation coefficient with the target. The first elbow occurs around 0.15; we will use this cutoff value to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_tol = 0.15\n",
    "\n",
    "core_vars = all_corr_abs_df.index[all_corr_abs_df['abs_corr'] >= corr_tol]\n",
    "\n",
    "X_core = X[core_vars]\n",
    "\n",
    "X_core.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-fold Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_core, y, test_size=2000, random_state=314)\n",
    "\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=1000, random_state=314)\n",
    "\n",
    "len(X_train), len(X_test), len(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_scaler.fit_transform(X_train.astype(float))\n",
    "X_test_scaled = X_scaler.transform(X_test.astype(float))\n",
    "X_valid_scaled = X_scaler.transform(X_valid.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first assign a rank to each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=314)\n",
    "\n",
    "rfe = RFE(estimator=logit, n_features_to_select=1)\n",
    "\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "ranking[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(rfe.ranking_, core_vars))[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vars = sorted(zip(rfe.ranking_, core_vars))\n",
    "\n",
    "len(sorted_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vars = 25\n",
    "auc_train_all, auc_test_all = [], []\n",
    "\n",
    "for num_preds in range(max_vars):\n",
    "    \n",
    "    ranked_preds = sorted_vars[:num_preds+1]\n",
    "    \n",
    "    preds = [x[1] for x in ranked_preds]\n",
    "    #print (preds)\n",
    "    \n",
    "    _X_train_scaled = X_scaler.fit_transform(X_train[preds].astype(float))\n",
    "    _X_test_scaled = X_scaler.transform(X_test[preds].astype(float))\n",
    "\n",
    "    logit.fit(_X_train_scaled, y_train)\n",
    "    \n",
    "    logit_scores_train = logit.predict_proba(_X_train_scaled)[:, 1]\n",
    "    logit_scores_test = logit.predict_proba(_X_test_scaled)[:, 1]\n",
    "    \n",
    "    auc_train = roc_auc_score(y_train, logit_scores_train)\n",
    "    auc_test = roc_auc_score(y_test, logit_scores_test)\n",
    "    \n",
    "    auc_train_all.append(auc_train)\n",
    "    auc_test_all.append(auc_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (12, 9))\n",
    "\n",
    "plt.plot(range(max_vars), auc_train_all, lw=2, label='Train')\n",
    "plt.plot(range(max_vars), auc_test_all, lw=2, label='Test')\n",
    "\n",
    "plt.xlabel('Number of Predictors', fontsize=14)\n",
    "plt.ylabel('AUC', fontsize=14)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('dark')\n",
    "\n",
    "diff = [a - b for a, b in zip(auc_train_all, auc_test_all)]\n",
    "\n",
    "f, ax = plt.subplots(figsize = (12, 9))\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "ax.plot(range(max_vars), auc_train_all, lw=2, label='Train', linestyle = '--', color='royalblue')\n",
    "ax.plot(range(max_vars), auc_test_all, lw=2, label='Test', color='royalblue')\n",
    "\n",
    "ax2.plot(range(max_vars), diff, lw=1, color = 'orangered', label = 'Difference')\n",
    "\n",
    "ax.plot([13, 13], [0.7, 1], color='gray', lw=.5)\n",
    "\n",
    "ax.set_xlabel('Number of Predictors', fontsize = 14)\n",
    "ax.set_ylabel('AUC', fontsize = 14)\n",
    "ax2.set_ylabel('Difference in AUC', fontsize = 14)\n",
    "ax.set_ylim([.7, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_preds = sorted_vars[:13]\n",
    "    \n",
    "final_preds = [x[1] for x in ranked_preds] \n",
    "\n",
    "len(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can combine Train and Test sets to build the final model\n",
    "\n",
    "X_train_test = X_train.append(X_test)\n",
    "y_train_test = y_train.append(y_test)\n",
    "\n",
    "X_train_test.shape, y_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.fit_transform(X_train_test[final_preds].astype(float))\n",
    "X_valid_scaled = X_scaler.transform(X_valid[final_preds].astype(float))\n",
    "\n",
    "logit.fit(X_train_scaled, y_train_test)\n",
    "\n",
    "logit_scores_train = logit.predict_proba(X_train_scaled)[:, 1]\n",
    "logit_scores_valid = logit.predict_proba(X_valid_scaled)[:, 1]\n",
    "\n",
    "auc_train = roc_auc_score(y_train_test, logit_scores_train)\n",
    "auc_valid = roc_auc_score(y_valid, logit_scores_valid)   \n",
    "\n",
    "print(auc_train, auc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe to save the model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_coeff = pd.DataFrame(list(zip(final_preds, [x[0] for x in np.transpose(logit.coef_)])))\n",
    "\n",
    "model_pred_coeff.columns=[['Predictor', 'Coefficient']]\n",
    "\n",
    "model_pred_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_coeff.loc[13]= ['Intercept', logit.intercept_[0]]\n",
    "\n",
    "model_pred_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_coeff.to_csv(path + r'model_coeff.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Calculate the false positive and true positive rates\n",
    "logit_fpr_train, logit_tpr_train, _ = roc_curve(y_train_test, logit_scores_train)\n",
    "logit_fpr_valid, logit_tpr_valid, _ = roc_curve(y_valid, logit_scores_valid)\n",
    "\n",
    "plt.figure().set_size_inches(12, 9)\n",
    "\n",
    "plt.plot(logit_fpr_train, logit_tpr_train, c='royalblue', lw=2, alpha=0.3, linestyle = '--',\n",
    "         label='Training (AUC = %0.3f)' %auc_train)\n",
    "\n",
    "plt.plot(logit_fpr_valid, logit_tpr_valid, c='royalblue', lw=2, \n",
    "         label='Validation (AUC = %0.3f)' %auc_valid)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='-', alpha=.5)\n",
    "plt.xlabel('False Positive Rate', fontsize = 14)\n",
    "plt.ylabel('True Positive Rate', fontsize = 14)\n",
    "plt.title('Ad Identification Model', fontsize = 16)\n",
    "plt.legend(loc=\"lower right\", fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = logit.predict(X_valid_scaled)\n",
    "\n",
    "pd.crosstab(y_valid, preds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.crosstab(y_valid, preds_valid) / len(y_valid)\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "\n",
    "plt.figure().set_size_inches(12, 9)\n",
    "\n",
    "sns.distplot(logit_scores_valid);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode Lift and Gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_scores_valid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_df = pd.DataFrame(data=logit_scores_valid, columns=['score'])\n",
    "\n",
    "lift_df['y'] = y_valid.values\n",
    "\n",
    "lift_df = lift_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "lift_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_df['cumul_y'] = lift_df['y'].cumsum()\n",
    "\n",
    "lift_df['cumul_pctg_y'] = lift_df['cumul_y'] / sum(lift_df['y'])\n",
    "\n",
    "lift_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_df = lift_df.reset_index()\n",
    "\n",
    "lift_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_df['rank'] = lift_df.index + 1\n",
    "\n",
    "lift_df['rank'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_df['decile'] = pd.qcut(lift_df['rank'], 10, labels=False)\n",
    "\n",
    "lift_df['decile'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_df.groupby('decile')['y'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Model Lift by decile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_df.groupby('decile')['y'].mean() / lift_df['y'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Model Gains by decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum = lift_df.groupby('decile')['y'].sum().cumsum() / lift_df['y'].sum()\n",
    "\n",
    "y_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_preds = [x for x in final_preds if x != 'width_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "\n",
    "for binary_pred in binary_preds:\n",
    "    \n",
    "    attr_values = df[binary_pred]\n",
    "    \n",
    "    y_rates = df.groupby([binary_pred])[target].mean().reset_index()\n",
    "    x_pctg = df[binary_pred].value_counts() / len(df)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = x_pctg.plot(kind='bar', color='royalblue', alpha=.5)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(['N' if x==0 else 'Y' for x in y_rates[binary_pred]], y_rates[target], lw=3, color = 'tomato')\n",
    "    \n",
    "    plt.title(binary_pred, fontsize=14)\n",
    "    ax.set_xlabel(target, fontsize = 14)\n",
    "    ax.xaxis.set_tick_params(rotation=0)\n",
    "    ax.set_ylabel('% of Records', fontsize = 14)\n",
    "    ax2.set_ylabel('Target Rate', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the continous predictor, you can first create bins and then create the profile plots. I will leave this as an exercise for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the scored dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[final_preds].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X_scaler.transform(X[final_preds].astype(float))\n",
    "\n",
    "X['Predicted Probability'] = logit.predict_proba(X_scaled)[:, 1]\n",
    "X['Predicted Class'] = logit.predict(X_scaled)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Predicted Probability'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(path + r'\\scored_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor means and standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_stats = pd.DataFrame(list(zip(X_train[final_preds].mean(), X_train[final_preds].std())), \n",
    "                          index=[final_preds],\n",
    "                          columns=['mean', 'stdev'])\n",
    "\n",
    "pred_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for the scoring process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, rec in pred_stats.iterrows():\n",
    "    pred = i[0]\n",
    "    mean = rec['mean']\n",
    "    stdev = rec['stdev']\n",
    "    print (\"df['%s']\" %pred, '= [(x - ', mean, ') /', stdev, \"for x in df['%s']\" %pred, ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, rec in model_pred_coeff.iterrows():\n",
    "    \n",
    "    pred = rec['Predictor'][0]\n",
    "    coeff = rec['Coefficient'][0]\n",
    "    \n",
    "    if pred != 'Intercept':\n",
    "            print (\"df['\"+pred+\"'] *\", coeff, '+')\n",
    "    else:\n",
    "        print (coeff)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
